---
title: "PUV_DEP_VariousMethods_loweGFRvsControl"
author: "Xin Wang"
date: "2025-03-25"
output: html_document
---

# loading the library
```{r setup, include=FALSE}
library(MSnbase)

#library(diann)
# BiocManager::install("GMSimpute")
#BiocManager::install("SeqKnn")
# BiocManager::install("rrcovNA")
# BiocManager::install("NormalyzerDE")
# BiocManager::install("limma")
#BiocManager::install("DEqMS")
library(GMSimpute)
#library(SeqKnn)

library(tidyverse)
library(ggplot2)
library(ggnewscale)
library(ggrepel)
#BiocManager::install("pRoloc")

library(edgeR)
library(clusterProfiler)
library(org.Mm.eg.db)

# loading the different methods of differentially expressed analyses of proteomic datasets
library(rrcovNA)
library(NormalyzerDE)
library(limma)
library(DEqMS)
#keytypes(org.Mm.eg.db) 
# BiocManager::install("DEP")
library("DEP")
# BiocManager::install("proDA")
library(proDA)
library(MSstats)
# BiocManager::install("ROTS")
library(ROTS)

library("pRoloc")
library(enrichplot)
library(tidyverse)
library(factoextra)
#BiocManager::install("org.Hs.eg.db",force = TRUE)
#BiocManager::install("ProteoMM")
library(ProteoMM)
library(org.Hs.eg.db)

```

# set up the directory and inputs
```{r setup, include=FALSE}
## read the table
#rm(list= ls())
setwd("/Users/XXW004/Documents/Projects/DarylMcleod/Results/Results_03242025/Intensity_Analyses_LowGFRVsControls/")

OutDir <- c("/Users/XXW004/Documents/Projects/DarylMcleod/Results/Results_03242025/Intensity_Analyses_LowGFRVsControls/")
## set up the input dir

InputDir <- c("/Users/XXW004/Documents/Projects/DarylMcleod/Results/Results_03242025/Intensity_Analyses_LowGFRVsControls/")

# the input dir for clinical characteritic
ClinicInputDir<- c("/Users/XXW004/Documents/Projects/DarylMcleod/Data Analysis/")

# the input dir for Raw reads
RawInputDir <- c("/Users/XXW004/Documents/Projects/DarylMcleod/RawData/Final_Used/")
# Here we applied various statitical tools for the protein intensity results generated by Fragpipe

# the folder for the DEqMS results
# https://www.bioconductor.org/packages/release/bioc/vignettes/DEqMS/inst/doc/DEqMS-package-vignette.html
DEqMSDir <- paste0(OutDir, "DEqMS/")
# https://www.bioconductor.org/packages/devel/bioc/vignettes/DEP/inst/doc/DEP.html

# the folder for the ProteoMM #https://www.bioconductor.org/packages/release/bioc/vignettes/ProteoMM/inst/doc/ProteoMM_vignette.html
ProteoMMDir<- paste0(OutDir,"ProteoMM")
DEPDir <- paste0(OutDir, "DEP/")
# the folder for the Msstats results
# https://www.bioconductor.org/packages/release/bioc/html/MSstats.html
MSstatsDir <- paste0(OutDir, "MSstats/")
# the folder for the t-test results
TtestDir <- paste0(OutDir, "Ttest/")
# the folder for the ANNOVA results
ANNOVADir <- paste0(OutDir, "ANNOVA/")
# the folder for the Limma results
LimmaDir <- paste0(OutDir, "Limma/")
# the folder for the proDA results
proDADir<- paste0(OutDir, "proDA/")

# the folder for the ROTS results
# https://bioconductor.org/packages/devel/bioc/vignettes/ROTS/inst/doc/ROTS.pdf

ROTSDir<- paste0(OutDir,"ROTS/")
WilcoxonTestDir <- paste0(OutDir,"Wilcoxon/")

dir.create(DEqMSDir, showWarnings = FALSE)
dir.create(DEPDir, showWarnings = FALSE)
dir.create(MSstatsDir, showWarnings = FALSE)
dir.create(TtestDir, showWarnings = FALSE)
dir.create(ANNOVADir, showWarnings = FALSE)
dir.create(LimmaDir, showWarnings = FALSE)
dir.create(proDADir, showWarnings = FALSE)
dir.create(ROTSDir, showWarnings = FALSE)
dir.create(ProteoMMDir, showWarnings = FALSE)
dir.create(WilcoxonTestDir, showWarnings = FALSE)
# setting up the seed 

set.seed(1000000)


```

# we read the clinical characteristics of these samples
```{r}
# read the clinical characteristic of the sample

CaseClinicalCharacters<- read.csv(paste0(ClinicInputDir, "ClinalFactor_FromLindsey2.csv" ))

ControlClinicalCharacters<- read.csv(paste0(ClinicInputDir, "ControlClinic2.csv" ))
CaseClinicalCharacters

```

```{r}
# change the case and control id # we generated the ID 
SampleIDTransfer<- read.csv(paste0(RawInputDir,"SampleID_Transfer.csv"))

SampleIDTransfer

# For the comparison samples
RequestedName<- c(paste0("Case",1:10),paste0("Control",1:20))

RequestedNameJD<- SampleIDTransfer %>% filter(ID.By.MS %in% RequestedName) %>% pull(ID.By.JD)
# 
```

# we also generated peptide counts per protein from combined_peptide.tsv 
Each row in combined_peptide.tsv represents a peptide (not PSM), and contains:
A unique peptide sequence
One or more associated proteins (in a column like Protein or Protein(s))
Intensity or spectral data across samples (optional)
We tried to measure the min peptide counts for the groups

```{r}
library(data.table)
# 
# pep <- fread(paste0(RawInputDir,"combined_peptide.tsv"))
# 
# colnames(pep)
# 
# pep %>%  head()
# pep %>%
#   separate_rows(Protein, sep = ";") %>% head()
# # Proteins might be semicolon-separated — split them and count peptides per protein:
# # Expand rows: split protein assignments
# 
# # Check for correct protein column name (might be "Protein" or "Protein.Ids")
# protein_col <- grep("Protein", colnames(pep), value = TRUE)[1]
# # Count number of PSMs per protein
# as.data.frame(table(pep[[protein_col]]))
# 
# psm_count <- as.data.frame(table(pep[[protein_col]]))
# colnames(psm_count) <- c("Protein", "PSM_count")
# 
# pep_expanded <- pep %>%
#   separate_rows(Protein, sep = ";") %>%
#   distinct(Protein, `Peptide Sequence`)  # count unique peptide per protein
# 
# pep_expanded
# # Count unique peptides per protein
# peptide_count <- pep_expanded %>%
#   group_by(Protein) %>%
#   summarise(peptides = n_distinct(`Peptide Sequence` )) %>%
#   ungroup() %>% arrange(peptides)
# peptide_count

# we load PSM data

# 

psm_counts <- list()
PSMIndir <- c("/Users/XXW004/Library/CloudStorage/OneDrive-NationwideChildren'sHospital/Proteomics/RawProteomicDatasets/FullResults/")
Samplelabel<- c(paste0("Case_", 1:20), paste0("Control_",21:40))
Samplelabel
# create a functon to read the process one file

process_psm<- function(sample_name){
  CasePsm <- fread(paste0 (PSMIndir, sample_name, "/", "psm.tsv"))
  # Optional: remove contaminants or reverse hits
  CasePsm <- CasePsm[!grepl("Contaminant|Reverse", Protein),]
  # Step 1: Expand multi-protein assignments (split semicolon-separated IDs)
  psm_expanded <- separate_rows(CasePsm, Protein, sep = ";")
  
  # Step 2: Count PSMs per protein per sample
  name <- paste0(sample_name, "_psm")
  psm_expanded %>%
    group_by(Protein) %>%
    summarise(!!name := n(), .groups = "drop")
  
}

# Process all samples and combine
psm_counts <- map(Samplelabel, process_psm)

# then combine all the lists
psm_expanded <-Reduce(function(x, y) merge(x, y, by = "Protein", all = TRUE), psm_counts)

# Replace NA with 0
psm_expanded[is.na(psm_expanded)] <- 0
# we then calculate the minumum PSM per protein across all samples

psm_cols <- grep("_psm", colnames(psm_expanded), value = TRUE)
psm_cols
psm_expanded[,psm_cols]

# extract the min value
psm_expanded$min_psm<-apply(psm_expanded[,psm_cols],1, function(x) min(as.numeric(x), na.rm = TRUE))

# extract the total value
psm_expanded$sum_psm<-apply(psm_expanded[,psm_cols],1, function(x) sum(as.numeric(x), na.rm = TRUE))
# save into a psm file
write.csv(psm_expanded,"combine_protein_peptidenumber.csv")

# we need to change the name into the now name
grep("Control_|Case_",colnames(psm_expanded),value=T)

# 
colnames(psm_expanded)
Old_name<- paste0(SampleIDTransfer$ID.By.JD, "_psm")
New_name<- paste0(SampleIDTransfer$ID.By.MS, "_psm")

# create the name mapping
name_map<- setNames(Old_name, New_name)

# then apply the rename
psm_expanded<- psm_expanded %>% rename(!!!name_map)
write.csv(psm_expanded,"combine_protein_peptidenumber_rename.csv")

# add the specific columns min values
psm_expanded
# for selected column

SamplelabelSelectednlowGFR <- c(paste0("Case", 1:10,"_psm"), paste0("Control",1:20,"_psm"))
#SamplelabelSelectedlowGFR <- c(paste0("Case", 1:20,"_psm"))
colnames(psm_expanded)
SamplelabelSelectednlowGFR
psm_expanded$min_pep_lowGFR <- apply (psm_expanded[,SamplelabelSelectednlowGFR],1, function(x) min(x))


write.csv(psm_expanded,"combine_protein_peptidenumber_rename.csv")


#psm_expanded %>% head()
```

# Reading the object that proproceeded with the filteration, imputation for the MaxLFQ RDS from FragPipe
# Note, we only selected the datasets for low eGFR <90 and compare to controls

# address the missing values,
```{r}
# We have previously generated the RDS that have preproceeded with filtering, Log transfer, imputation for MaxLFQ Intensity (We used the without any normalization)
# dat.obj = readRDS(paste0(InputDir, "PUV_knnimputation_withoutnormalization_intens_imp.RDS"))
#dat.obj = normalise(dat.obj, "vsn")
# extract the intensity values of each samples
# read the original protein with spectra counts 
pro_file<-read.csv(paste0(RawInputDir,"combined_final_protein_ByJF.csv"), header = T)
head(pro_file)
length(colnames(pro_file))
colnames(pro_file)
#########################################################################################
### 1. Preprocessing the protemic dataset, including:
###             1.1 Rename the protein name:
###             1.2 Filtered the proteins that showed less than 5 samples expressed.
###             1.3 Log tranfered and normalization: here we applied the quantile normalization
#########################################################################################
#
################################################################
##  1.1 Rename the protein name: filter the proteins not detected as homo sapiens; change empty gene name to the Entry.Name and remove the _HUMAN substrings; make the protein name unique by adding _1, _2 using make.unique
### 1.1.1 we filtered the proteins that are only in the human. Note we detected some of proteins that in Ovis arises and Sus crofa, therefore, we ignore these dataset
pro_file_update<- pro_file %>% filter(Organism == "Homo sapiens") 
### 1.1.2 we change empty gene name to the Entry.Name and remove the _HUMAN substrings; make the protein name unique by adding _1, _2 using make.unique
#The dataset has unique Uniprot identifiers, however those are not immediately informative. The associated gene names are informative, however these are not always unique.
# Are there any duplicated gene names?
pro_file_update$Gene %>% duplicated() %>% any()
# Make a table of duplicated gene names
pro_file_update %>% group_by(Gene) %>% summarize(frequency = n()) %>%
  arrange(desc(frequency)) %>% filter(frequency > 1)
# Make unique names using the annotation in the "Gene.names" column as primary names and the annotation in "Protein.IDs" as name for those that do not have an gene name.

# some of genes are not well annotated, we used the entry name to fix the name
pro_file_update$Gene <- ifelse(is.na(pro_file_update$Gene) | pro_file_update$Gene == "", pro_file_update$Entry.Name, pro_file_update$Gene)
# we replace the name with _HUMAN to ''
pro_file_update$Gene<- gsub("_HUMAN","",pro_file_update$Gene)
# make the name unique
pro_file_update$Gene<- make.unique(pro_file_update$Gene)
### 1.1.3 we check the final Gene name, whether contain HUMAN, adding _1, _2 and whether they are unique
table(grepl("_HUMAN",pro_file_update$Gene))
pro_file_update$Gene %>% duplicated() %>% any()
pro_file_update %>% group_by(Gene) %>% summarize(frequency = n()) %>%
  arrange(desc(frequency)) %>% filter(frequency > 1)
# we create a column taht to add _1 _2, _3 for 
# pro_file %>%
#   group_by(Gene) %>%
#   mutate(Unique_Protein = ifelse(n() > 1, paste0(Gene, "_", row_number()), Gene))
# 
################################################################
## 1.2 Filtered the proteins that showed less than 5 samples expressed.
#  1.2.1 extract the intensity input, we used the unique intensity to make the final input
#  1.2.2 select the samples that only have high eGFR >=90
#pro_file_update

intensOri<-pro_file_update[, 175:length(colnames(pro_file_update))]
intensOri
# put the 0 intensity value as NA
intensOri[intensOri==0]<-NA
row.names(intensOri) <-pro_file_update[,4]

# we remove the string .MaxLFQ.Intensity from the colnames
colnames(intensOri)<-gsub('.MaxLFQ.Intensity','',colnames(intensOri))

# we reorder the 
Desired_NameOrder <- c("Case1","Case2","Case3","Case4","Case5","Case6","Case7","Case8","Case9","Case10","Control1","Control2","Control3","Control4","Control5","Control6","Control7","Control8","Control9","Control10","Control11","Control12","Control13","Control14","Control15","Control16","Control17","Control18","Control19","Control20")
# we only select the samples have the desired_name
intens <- intensOri[,Desired_NameOrder] 
intensOri["UPK2",]
# we filtered the samples ~0.9
idx_retain<-which(apply(intens,1,function(x) sum(is.na(x)))<length(intens[1,])*0.9)
intens=intens[idx_retain,Desired_NameOrder]

# check the filter whether contain UPK2
# the requested protein
RequestedProteins<- "UPK2"
intens[RequestedProteins,]
################################################################

################################################################
## 1.3 Log tranfered and normalization
# Here we consider to use the log2 to transfer the dataset
intens_tran<-log2(as.matrix(intens))
intens_tran[RequestedProteins,]
table(is.na(intens_tran))
intens_tran[RequestedProteins,]

# as the sample have already been normalized, we did not perform the step
boxplot(intens_tran,las=2,main="PUV proteomic datasets")

# we perform the normalization by 
intens_tran_normalized = normalizeBetweenArrays(intens_tran, method="quantile")
intens_tran_normalized[RequestedProteins,]
?normalizeBetweenArrays

# We perfomed the imputation using Knn
# get the protein names:
fd <- data.frame(row.names(intens_tran_normalized))
row.names(fd)<-row.names(intens_tran_normalized)
fd
# get the sample names:
pd <- data.frame(colnames(intens_tran_normalized))
row.names(pd) = colnames(intens_tran_normalized)
pd
# imputation using the MSnSet 
intens_imp<-MSnSet(intens_tran_normalized, fd, pd)
intens_imp<- MSnbase::impute(intens_imp, "knn")

table(is.na(intens_imp))

# check the expression of these samples using the MaxLFQ density
dat.log.exp  <- intens_imp@assayData[["exprs"]]
dat.log.exp %>%  head()
# 
# 
#  # we have already log tranfer the dataset
# 
# # we then use the quanitfile normalization
 #intens_imp.vsn <- normalise(intens_imp, "vsn")
# 
# exprs(intens_imp.vsn) 
# then plot the PCA results
 intens_imp@assayData$exprs
 
# DEG_Intens before the log transfer for the raw filtered intens
library(reshape2)
IntensData<- intens %>%  rownames_to_column("ProteinID")
DEG_Intens_long<- reshape2::melt(IntensData, id.vars = "ProteinID")

DEG_Intens_long %>%  head()
# plot density

ggplot(DEG_Intens_long, aes(x = value, color = variable, fill = variable)) +
  geom_density(alpha = 0.3) +
  theme_minimal() +
  labs(title = "Density Plots for Each Sample without Log2 for high eGFR", x = "Value", y = "Density")

# generate the density distribution after log2 and imputation
dat.log.exp  <- intens_imp@assayData[["exprs"]]
dat.log.exp
dat.log.exp.long <- reshape2::melt(dat.log.exp)


# 
dat.log.exp.long %>%  head()
ggplot(dat.log.exp.long, aes(x = value, color = Var2, fill = Var2)) +
  geom_density(alpha = 0.3) +
  theme_minimal() +
  labs(title = "Density Plots for Each Sample with Log2 and imputation for high eGRF", x = "Value", y = "Density")
 
##### generate a PCA analysese

colorDiff<-c(rep("red",10), rep("blue",20))

mds <- plotMDS(dat.log.exp, col=colorDiff)
toplot <- data.frame(Dim1 = mds$x, Dim2 = mds$y)
library(ggplot2)
#ggplot(toplot, aes(Dim1, Dim2))+ geom_point() +
# geom_text_repel(label=label_name,segment.size=0.01,hjust=0.2, size=3,  col=c(rep("gray",5),rep("black",4),rep("blue",4), rep("red",4), rep("yellow",4),rep("violet",4),rep("brown",4),rep("blue",4),))+
#  theme_classic()
#geom_text( label = label_name, nudge_x = 0.1, nudge_y = 0.1,  check_overlap = T)+
#theme_bw() + 
#  theme(panel.border = element_blank(), panel.grid.major = element_blank(),panel.grid.minor = element_blank(), axis.line = element_line(colour = "black"))
#Gcol<-c(rep("gray",5),rep("black",4),rep("blue",4), rep("red",4),rep("brown",4),rep("yellow",4),rep("navy blue",3), rep("purple",4),rep("LimeGreen",4),rep("pink",4),rep("orange",4), rep("magenta",4))
pdf("LowGFR_Controls_PCAanalyses.pdf", width = 6, height = 6)
ggplot(toplot, aes(Dim1, Dim2))+ geom_point(col=colorDiff) +
  geom_text_repel(label=colnames(dat.log.exp),segment.size=0.01,hjust=0.2, size=3, col=colorDiff,max.overlaps=20)+
  theme_classic()+
  #geom_text( label = label_name, nudge_x = 0.1, nudge_y = 0.1,  check_overlap = T)+
  theme_bw() + 
  theme(panel.border = element_blank(), panel.grid.major = element_blank(),panel.grid.minor = element_blank(), axis.line = element_line(colour = "black"))

dev.off()

dat.log.exp = na.omit(dat.log.exp)
# we used the median centered
boxplot(dat.log.exp, main ="PUV proteomic")

# We generate the final expressed dataset, this dataset have already performed the filtering, pimputation. 
str(dat.log.exp)


```


# run with the DEqMS
```{r}
# 2. the method of DEqMS
# Here we used quant data columns for DEqMS
# the 
setwd(dir = DEqMSDir)
# 2.1.1 preprocess the data
# due to the data is not median centered, we normalized the based on median normalization
#Here we used the filtered datasets
dat.log.exp %>% head()

boxplot(dat.log.exp, main ="PUV proteomic")
rownames(dat.log.exp)
# 2.1.2 Make design table
cond = as.factor(c(rep("Case", 10), rep("Controls",20)))

# the function mdoel is use the generate the design matrix
design = model.matrix(~0+ cond)
colnames(design) = gsub("cond","",colnames(design))
colnames(design)

# 2.1.3 make contrasts
x<- c("Case-Controls")
contrast =  makeContrasts(contrasts=x,levels=design)
contrast
fit1 <- lmFit(dat.log.exp, design)
fit2 <- contrasts.fit(fit1,contrasts = contrast)
fit3 <- eBayes(fit2)
print(fit3)
topTable(fit3)

# the limma results
FinalLimmaResults<- topTable(fit3, coef=1, adjust="fdr", number=Inf)

write.csv(FinalLimmaResults, paste(LimmaDir,"PUV_Limma_results_LowGFRPUVvsControl.csv"))

# The above shows Limma part, now we use the function spectraCounteBayes in DEqMS to correct bias of variance estimate based on minimum number of psms per protein used for quantification.We use the minimum number of PSMs used for quantification within and across experiments to model the relation between variance and PSM count.(See original paper)
# 2.1.4 Integrate Spectral count information for DEqMS analysis

# assign a extra variable `count` to fit3 object, telling how many PSMs are quantifed for each protein

# Here we used the peptide counts, note that it is not the spetrum counts
library(matrixStats)

# 
pro_file_update %>% head()
# # extract the count table, we used the Combined Total Peptides column, please note that we should use the peptide number instead of the spectrum number. 
# count_table<-as.data.frame(pro_file_update[,c(4,11)]) ### this value is important 
# count_table %>% head()
# # we should set up the row names as the protein
# row.names(count_table) <-count_table$Gene
# count_table %>% head()
# # we remove the string .MaxLFQ.Intensity from the colnames
# colnames(count_table)<-gsub('.Unique.Spectral.Count','',colnames(count_table))
# 
# # count_columns 
# # we reorder the 
# Desired_NameOrder <- c("Case1","Case2","Case3","Case4","Case5","Case6","Case7","Case8","Case9","Case10","Case11","Case12","Case13","Case14","Case15","Case16","Case17","Case18","Case19","Case20","Control1","Control2","Control3","Control4","Control5","Control6","Control7","Control8","Control9","Control10","Control11","Control12","Control13","Control14","Control15","Control16","Control17","Control18","Control19","Control20")
# # we filtered the samples ~0.9
# #idx_retain<-which(apply(intens,1,function(x) sum(is.na(x)))<length(intens[1,])*0.9)
# length(rownames(fit3))
# count_table=count_table[rownames(fit3),Desired_NameOrder]
# count_table %>% nrow()
# check the protein number in each sample
ProteinGeneTransfer<- pro_file_update[,c(1,4)]
psm_expanded <- psm_expanded %>% left_join(ProteinGeneTransfer, by = "Protein")
psm_expanded %>% head()
colnames(psm_expanded)

rownames(fit3)
count_table
psm_expanded %>% filter(Gene %in% rownames(fit3)) 
# please note that we should only keep the proteins that used for the limma analyses.
count_table= psm_expanded %>% filter(Gene %in% rownames(fit3)) 
rownames(count_table) = count_table$Gene
length(rownames(fit3))
count_table
length(count_table$min_pep_lowGFR) ## to be consistent.
rownames(count_table)
rownames(fit3$coefficients)
rownames(fit3$coefficients)
# set up the protein counts.
fit3$count = count_table[rownames(fit3$coefficients),"min_pep_lowGFR"]
fit3
# Remove missing spectral counts

dim(fit3$coefficients) 
# try adding a pseudo count 1 to all values before using spectraCounteBayes function.if the minimum PSM count is 0, you can add a pseudo count 1 to it.
fit3$count=  fit3$count+1
fit4 = spectraCounteBayes(fit3)
VarianceBoxplot(fit4)
# extract the results as data frame and svae it
# the DEqMS results
DEqMS.results = outputResult(fit4,coef_col = 1)
DEqMS.results %>% head()


DEqMS.results %>% dplyr::filter(sca.P.Value <=0.05) %>% nrow()
# the above method is not a good way to detect the DEPs using the peptide

DEqMS.results$log.sca.pval = -log10(DEqMS.results$sca.P.Value)
ggplot(DEqMS.results, aes(x = logFC, y =log.sca.pval )) + 
    geom_point(size=0.5 )+
    theme_bw(base_size = 16) + # change theme
    xlab(expression("log2(miR372/ctrl)")) + # x-axis label
    ylab(expression(" -log10(P-value)")) + # y-axis label
    geom_vline(xintercept = c(-1,1), colour = "red") + # Add fold change cutoffs
    geom_hline(yintercept = 3, colour = "red") + # Add significance cutoffs
    geom_vline(xintercept = 0, colour = "black") + # Add 0 lines
    scale_colour_gradient(low = "black", high = "black", guide = FALSE)+
    geom_text_repel(data=subset(DEqMS.results, abs(logFC)>1&log.sca.pval > 3),
                    aes( logFC, log.sca.pval ,label=gene)) # add gene label

fit4$p.value = fit4$sca.p
# volcanoplot highlight top 20 proteins ranked by p-value here
volcanoplot(fit4,coef=1, style = "p-value", highlight = 20,
            names=rownames(fit4$coefficients))
VarianceBoxplot(fit4,n=30,main="TMT10plex dataset PXD004163",xlab="PSM count")
VarianceScatterplot(fit4,main="TMT10plex dataset PXD004163")
# comparing DEqMS to other methods
VarianceScatterplot(fit3, xlab="log2(PSM count)")
limma.prior = fit3$s2.prior
abline(h = log(limma.prior),col="green",lwd=3 )
legend("topright",legend=c("DEqMS prior variance","Limma prior variance"),
        col=c("red","green"),lwd=3)

op <- par(mfrow=c(1,2), mar=c(4,4,4,1), oma=c(0.5,0.5,0.5,0))
Residualplot(fit3,  xlab="log2(PSM count)",main="DEqMS")
x = fit3$count
y = log(limma.prior) - log(fit3$sigma^2)
plot(log2(x),y,ylim=c(-6,2),ylab="Variance(estimated-observed)", pch=20, cex=0.5,
     xlab = "log2(PSMcount)",main="Limma")
# Posterior variance comparison between DEqMS and Limma
library(LSD)
op <- par(mfrow=c(1,2), mar=c(4,4,4,1), oma=c(0.5,0.5,0.5,0))
x = fit3$count
y = fit3$s2.post
heatscatter(log2(x),log(y),pch=20, xlab = "log2(PSMcount)", 
     ylab="log(Variance)",
     main="Posterior Variance in Limma")

y = fit3$sca.postvar
heatscatter(log2(x),log(y),pch=20, xlab = "log2(PSMcount)",
     ylab="log(Variance)", 
     main="Posterior Variance in DEqMS")

write.csv(DEqMS.results, "PUV_DEqMStest_results_LowGFRPUVvsControl.csv")

```

# T test
```{r}
setwd(TtestDir)
# in order to use T test, we test for nromality
shapiro.test(dat.log.exp[,3])
# apply the shapiro test to all sample
# clearly the T test is 
shapiro_results <- apply(dat.log.exp, 2, function(x) shapiro.test(x)$p.value)
shapiro_results
# As the data is not normally distributed, we will not consider parameteric test that assume normality (e.g., t-test, ANOVA) may not be appropriate. Our datasets showed clearly biomodal, we therefore applied Wilcoxon Rank sum test instead 


dat.log.exp %>% head()
pval.372 = apply(dat.log.exp, 1, function(x) 
t.test(as.numeric(x[c(1:10)]), as.numeric(x[c(11:30)]))$p.value)

logFC.372 = rowMeans(dat.log.exp[,c(1:10)])-rowMeans(dat.log.exp[,c(11:20)])


ttest.results = data.frame(gene=rownames(dat.log.exp),
                    logFC=logFC.372,P.Value = pval.372, 
                    adj.pval = p.adjust(pval.372,method = "fdr")) 
ttest.results
#ttest.results$PSMcount = psm.count.table[ttest.results$gene,"count"]
ttest.results = ttest.results[with(ttest.results, order(P.Value)), ]
head(ttest.results)
# save the results 
write.csv(ttest.results, "PUV_Ttest_results_LowGFRPUVvsControl.csv")

```

# Wilcoxon Rank Test for them
```{r}
WilcoxonTestDir
setwd(WilcoxonTestDir)
# in order to use T test, we test for nromality
shapiro.test(dat.log.exp[,3])
# apply the shapiro test to all sample
# clearly the T test is 
shapiro_results <- apply(dat.log.exp, 2, function(x) shapiro.test(x)$p.value)

# As the data is not normally distributed, we will not consider parameteric test that assume normality (e.g., t-test, ANOVA) may not be appropriate. Our datasets showed clearly biomodal, we therefore applied Wilcoxon Rank sum test instead 


dat.log.exp %>% head()
pval.wilcox = apply(dat.log.exp, 1, function(x) 
wilcox.test(as.numeric(x[c(1:10)]), as.numeric(x[c(11:20)]))$p.value)

logFC.372 = rowMeans(dat.log.exp[,c(1:10)])-rowMeans(dat.log.exp[,c(11:20)])


wilcox.results = data.frame(gene=rownames(dat.log.exp),
                    logFC=logFC.372,P.Value = pval.wilcox, 
                    adj.pval = p.adjust(pval.wilcox,method = "fdr")) 
wilcox.results
#ttest.results$PSMcount = psm.count.table[ttest.results$gene,"count"]
wilcox.results = wilcox.results[with(wilcox.results, order(P.Value)), ]

# we inner join two lists wilcox.results and dat.log.exp
Finalwilcox.results<- wilcox.results %>%  rownames_to_column("ID") %>% inner_join(dat.log.exp %>% as.data.frame() %>%  rownames_to_column("ID"), by ="ID") 

head(Finalwilcox.results)
# save the results 
write.csv(Finalwilcox.results, "PUV_Wilcoxtest_results_LowGFRPUVvsControl.csv")


```

# Annova analysis
Anova analysis is equivalent to linear model analysis. The difference to Limma analysis is that estimated variance is not moderated using empirical bayesian approach as it is done in Limma.
```{r}
# set up the path:
setwd(ANNOVADir)
ord.t = fit1$coefficients[, 1]/fit1$sigma/fit1$stdev.unscaled[, 1]
ord.p = 2*pt(abs(ord.t), fit1$df.residual, lower.tail = FALSE)
ord.q = p.adjust(ord.p,method = "BH")
anova.results = data.frame(gene=names(fit1$sigma),
                            logFC=fit1$coefficients[,1],
                            t=ord.t, 
                            P.Value=ord.p, 
                            adj.P.Val = ord.q)
anova.results
#psm.count.table
#anova.results$PSMcount = psm.count.table[anova.results$gene,"count"]
anova.results = anova.results[with(anova.results,order(P.Value)),]
head(anova.results)

write.csv(anova.results, file = "PUV_Annovar_results_LowGFRPUVvsControl.csv")
```

<!-- # Limma -->
<!-- Extract limma results using topTable function, coef = 1 allows you to extract the specific contrast (miR372-ctrl), option n= Inf output all rows. -->
<!-- ```{r} -->

<!-- limma.results = topTable(fit2,coef = 1,n= Inf) -->
<!-- limma.results$gene = rownames(limma.results) -->
<!-- #Add PSM count values in the data frame -->
<!-- limma.results$PSMcount = psm.count.table[limma.results$gene,"count"] -->

<!-- head(limma.results) -->
<!-- ``` -->

# Visualize the distribution of p-values by different analysis
plotting all proteins ranked by p-values.

```{r}
#DEqMS.results
plot(sort(-log10(FinalLimmaResults$P.Value),decreasing = TRUE), 
    type="l",lty=2,lwd=2, ylab="-log10(p-value)",ylim = c(0,40),
    xlab="Proteins ranked by p-values",
    col="purple")
lines(sort(-log10(DEqMS.results$sca.P.Value),decreasing = TRUE), 
        lty=1,lwd=2,col="red")
lines(sort(-log10(anova.results$P.Value),decreasing = TRUE), 
        lty=2,lwd=2,col="blue")
lines(sort(-log10(ttest.results$P.Value),decreasing = TRUE), 
        lty=2,lwd=2,col="orange")
legend("topright",legend = c("Limma","DEqMS","Anova","t.test"),
        col = c("purple","red","blue","orange"),lty=c(2,1,2,2),lwd=2)
dev.off()

# top 500 proteins
plot(sort(-log10(FinalLimmaResults$P.Value),decreasing = TRUE)[1:500], 
    type="l",lty=2,lwd=2, ylab="-log10(p-value)", ylim = c(2,10),
    xlab="Proteins ranked by p-values",
    col="purple")
lines(sort(-log10(DEqMS.results$sca.P.Value),decreasing = TRUE)[1:500], 
        lty=1,lwd=2,col="red")
lines(sort(-log10(anova.results$P.Value),decreasing = TRUE)[1:500], 
        lty=2,lwd=2,col="blue")
lines(sort(-log10(ttest.results$P.Value),decreasing = TRUE)[1:500], 
        lty=2,lwd=2,col="orange")
legend("topright",legend = c("Limma","DEqMS","Anova","t.test"),
        col = c("purple","red","blue","orange"),lty=c(2,1,2,2),lwd=2)

```

# DEP Analyses 
https://www.bioconductor.org/packages/devel/bioc/vignettes/DEP/inst/doc/DEP.html
# Attention: DEP will use the raw MaxLFQ intensity results
```{r}
# Data prepration for the DEP, we ignored the remove duplicated proteins and 
setwd(dir = DEPDir)

# Here we also use the orginal MAX LFQ as input
intens %>% head()


#as.data.frame(dat.log) %>% rownames_to_column("ID") %>% head()
DEG_Intens<- as.data.frame(intens) %>% rownames_to_column("ID") 
# get LFQ column number
LFQ_columns_number <- c(2:31)
# check the distribution
DEG_Intens$Case1
colnames(DEG_Intens)

# create a data frame for experimental design
experimental_design <- data.frame(
  label = colnames(DEG_Intens)[2:31],  # Sample names
  condition = as.factor(c(rep("Case", 10), rep("Controls",20))),  # Define conditions
  replicate = c(seq(1:10), seq(1:20)) # Biological replicates
)

DEG_Intens$name<- DEG_Intens[,1]

?make_se

data.se <- make_se(DEG_Intens, LFQ_columns_number, experimental_design)
plot_frequency(data.se)
dim(data.se)
colnames(data.se)
# Filter for proteins that are identified in 4 replicates of at least one condition
#data_filt <- filter_missval(data.se, thr = 4)
?filter_missval

#plot_numbers(data_filt)

#plot_coverage(data_filt)
# Plot intensity distributions and cumulative fraction of proteins with and without missing values
#plot_detect(data_filt)
# Less stringent filtering:
# Filter for proteins that are identified in 2 out of 3 replicates of at least one condition
# data_filt2 <- filter_missval(data.se, thr = 1)

# Normalization, we did not filter the proteins
data_norm <- normalize_vsn(data.se)
# Visualize normalization by boxplots for all samples before and after normalization
# plot_normalization(data_filt, data_norm)
# 
# # Impute data for missing values 
# plot_missval(data_filt)
# All possible imputation methods are printed in an error, if an invalid function name is given.

# Impute missing data using the k-nearest neighbour approach (for MAR)
data_imp_knn <- DEP::impute(data_norm, fun = "knn", rowmax = 0.9)

plot_imputation(data_norm, data_imp_knn)

# Differential enrichment analysis  based on linear models and empherical Bayes statistics

# Test every sample versus control
data_diff <- DEP::test_diff(data_imp_knn, type = "all")
data_diff %>% head()
dep <- add_rejections(data_diff, alpha = 0.05, lfc = 0)
  
dep
res <- get_results(dep)

res %>% head()
# Visualization of the results
plot_pca(dep, x = 1, y = 2, n = 500, point_size = 4)
# Plot the Pearson correlation matrix
plot_cor(dep, significant = TRUE, lower = 0, upper = 1, pal = "Reds")

plot_heatmap(dep, type = "centered", kmeans = TRUE, 
             k = 6, col_limit = 4, show_row_names = FALSE
             )

plot_heatmap(dep, type = "contrast", kmeans = TRUE, 
             k = 6, col_limit = 10, show_row_names = FALSE)
plot_volcano(dep, contrast = "Case_vs_Controls", label_size = 2, add_names = TRUE)

# Barplots of a protein of interest
#plot_single(dep, proteins = c("USP15", "IKBKG"))

# generate the results table
# Generate a results table
data_results <- get_results(dep)

# order by the p value
data_results = data_results[with(data_results,order(Case_vs_Controls_p.val)),]
data_results %>% head()
write.csv(data_results,"PUV_DEP_results_LowGFRPUVvsControl.csv")

```

# MSstats: 
https://www.bioconductor.org/packages/release/bioc/vignettes/MSstats/inst/doc/MSstatsWorkflow.html

We used the MSstats using MSstats.csv generated by Fragpipe.
https://fragpipe.nesvilab.org/docs/tutorial_msstats.html
```{r}
# set up the path:
setwd(MSstatsDir)

# please attention the replicate id is not consistent with the sample id before
Orignalraw <- read_csv(paste0(RawInputDir, "MSstats.csv"), na = c("", "NA", "0"))
Orignalraw
# we read the Msstats.csv
# Orignalraw$ProteinName <- factor(raw$ProteinName)
# Orignalraw$PeptideSequence <- factor(raw$PeptideSequence)

colnames(Orignalraw)
nrow(Orignalraw)
RequestedNameJD
# we filtered the samples from Condition (case) and BioReplicate
raw<- Orignalraw %>% mutate(FinalSample = paste0(Condition,"_",BioReplicate)) %>% filter (FinalSample %in% RequestedNameJD )
nrow(raw)

# Processing the adata using MSstats

processedData <- dataProcess(raw, logTrans = 10)

?dataProcess
head(processedData$FeatureLevelData)
head(processedData$ProteinLevelData)

# Data process Plots
dataProcessPlots(data=processedData, type="ProfilePlot", 
                 address = FALSE, which.Protein = "sp|P12109|CO6A1_HUMAN")
0# Quality control plot
dataProcessPlots(data=processedData, type="QCPlot", 
                 address = FALSE, which.Protein = "sp|P12109|CO6A1_HUMAN")
# Quantification plot for conditions
dataProcessPlots(data=processedData, type="ConditionPlot", 
                 address = FALSE, which.Protein = "sp|P12109|CO6A1_HUMAN")

# Modeling
# In this step we test for differential changes in protein abundance across conditions using a linear mixed-effects model. The model will be automatically adjusted based on your experimental design.
# 
# A contrast matrix must be provided to the model. Alternatively, all pairwise comparisons can be made by passing pairwise to the function. For more information on creating contrast matrices, please see the citation linked at the beginning of this document.
model = groupComparison("pairwise", processedData)
head(model$ModelQC)
head(model$ComparisonResult)

model$ComparisonResult %>% head()
#  groupComparisonPlot
groupComparisonPlots(
  model$ComparisonResult,
  type="Heatmap",
  sig = 0.05,
  FCcutoff = FALSE,
  logBase.pvalue = 10,
  ylimUp = FALSE,
  ylimDown = FALSE,
  xlimUp = FALSE,
  x.axis.size = 10,
  y.axis.size = 10,
  dot.size = 3,
  text.size = 4,
  text.angle = 0,
  legend.size = 13,
  ProteinName = TRUE,
  colorkey = TRUE,
  numProtein = 100,
  clustering = "both",
  width = 800,
  height = 600,
  which.Comparison = "all",
  which.Protein = "all",
  address = FALSE,
  isPlotly = FALSE
)

groupComparisonPlots(
  model$ComparisonResult,
  type="VolcanoPlot",
  sig = 0.05,
  FCcutoff = FALSE,
  logBase.pvalue = 10,
  ylimUp = FALSE,
  ylimDown = FALSE,
  xlimUp = FALSE,
  x.axis.size = 10,
  y.axis.size = 10,
  dot.size = 3,
  text.size = 4,
  text.angle = 0,
  legend.size = 13,
  ProteinName = TRUE,
  colorkey = TRUE,
  numProtein = 100,
  clustering = "both",
  width = 800,
  height = 600,
  which.Comparison = "Case vs Control",
  which.Protein = "all",
  address = FALSE,
  isPlotly = FALSE
)

# rank the results by the Pvalue

MSstasResults <- model$ComparisonResult %>% arrange(pvalue)

# we need to change the protein name into symbol
pro_file_update[,c(1,4)] %>% head()

MSstasResultsF<-MSstasResults %>% inner_join(pro_file_update[,c(1,4)], by="Protein")
nrow(MSstasResultsF)
MSstasResultsF %>% head()
# save into the file

write.csv(MSstasResultsF, "PUV_MSstats_results_LowGFRPUVvsControl.csv")


```


# proDA
Here we will use proDA to differentially abundant proteins in label-free mass spectrometry data. The main challenge of this data are the many missing values. The missing values don’t occur randomly but especially at low intensities. This means that they cannot just be ignored. Existing methods have mostly focused on replacing the missing values with some reasonable number (“imputation”) and then run classical methods. But imputation is problematic because it obscures the amount of available information. Which in turn can lead to over-confident predictions.
https://www.bioconductor.org/packages/release/bioc/vignettes/proDA/inst/doc/Introduction.html
```{r}
library(proDA)

setwd(proDADir)

# We used the preliminary datasets that has been already well structured
intens_tran ### There are lots of NA values, and have already log2 transfered

# Quality control 
barplot(colSums(is.na(intens_tran)),
        ylab = "# missing values",
        xlab = "Sample 1 to 40")
boxplot(intens_tran,
        ylab = "Intensity Distribution",
        xlab = "Sample 1 to 36")
# Note that, the intensity distribution is shifted upwards for samples which also have a large number of missing values (for example the last one). This agrees with our idea that small values are more likely to be missing. On the other hand, this also demonstrates why normalization methods such as quantile normalization, which distort the data until all the distributions are equal, are problematic. I will apply the more “conservative” median normalization, which ignores the missing values and transforms the values so that the median difference between the sample and average across all other samples is zero.
normalized_abundance_matrix <- median_normalization(intens_tran)
# The base R dist() function can not handle input data that contains missing values, so we might be tempted to just replace the missing values with some realistic numbers and calculate the distance on the completed dataset. But choosing a good replacement value is challenging and can also be misleading because the samples with many missing values would be considered too close.

# Instead proDA provides the dist_approx() function that takes either a fitted model (ie. the output from proDA()) or a simple matrix (for which it internally calls proDA()) and estimates the expected distance without imputing the missing values. In addition, it reports the associated uncertainty with every estimate. The estimates for samples with many missing values will be uncertain, allowing the data analyst to discount them.
da <- dist_approx(normalized_abundance_matrix) # dist_approx() returns two elements the mean of the estimate and the associated sd. In the next step I will plot the heatmap for three different conditions, adding the 95% confidence interval as text to each cell.
# check the row name

# Fit the probablitistic dropout model
# In the next step, we will fit the actual linear probabilistic dropout model to the normalized data. But before we start, I will create a data.frame that contains some additional information on each sample, in particular to which condition that sample belongs
colnames(normalized_abundance_matrix)
sample_info_df <- data.frame(
  name = colnames(normalized_abundance_matrix),  # Sample names
  condition = as.factor(c(rep("Case", 10), rep("Controls",20))),  # Define conditions
  replicate = c(seq(1:10), seq(1:20)) # Biological replicates
)

# Now we can call the proDA() function to actually fit the model. We specify the design using the formula notation, referencing the condition column in the sample_info_df data.frame that we have just created. In addition, I specify that I want to use the S2R condition as the reference because I know that it was the negative control and this way automatically all coefficients measure how much each condition differs from the negative control.

fit <- proDA(normalized_abundance_matrix, design = ~ condition, 
             col_data = sample_info_df, reference_level = "Controls")

fit
# Equivalent to feature_parameters(fit)
fit$feature_parameters %>% head()

# Identify differential abundance
test_res <- test_diff(fit, "conditionCase")
proDAResults<- test_res %>% arrange(pval)


# save into the file

write.csv(proDAResults, "PUV_proDA_results_LowGFRPUVvsControl.csv")

```

# ROTS 
```{r}
library(ROTS)

setwd(ROTSDir)

data("upsSpikeIn")

# we used the normalized datasets
dat.log.exp # with the imputation and normalization

groups =c(rep(0,10), rep(1,20))
results = ROTS(data = dat.log.exp, groups = groups , B = 10000 , K = 500 , seed = 1234)

names(results)
summary(results, fdr = 0.05)
plot(results, fdr = 0.05, type = "volcano")
plot(results, fdr = 0.05, type = "heatmap")


# save results into 
names(results)

# add the output into a data.frame
rownames(results$data)
ROTS.results = data.frame(gene=rownames(results$data),
                    logFC=results$logfc,P.Value = results$pvalue, 
                    adj.pval = results$FDR) 
ROTS.results<-ROTS.results %>% arrange(P.Value) 
saveRDS(results, "PUV_ROTS_results_PUVvsControl.rds")

write.csv(ROTS.results, "PUV_ROTS_results_LowGFRPUVvsControl.csv")
```

