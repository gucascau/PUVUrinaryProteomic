---
title: "PUV Differentially expression analyes between cases and controls using various methods"
author: "Xin Wang"
email: "xin.wang@nationwidechildrens.org"
date: "2025-05-02"
description: This script will applied seven different proteomic analyses pipelines for PUV datasets. 
input: The input datasets were combined_final_protein.csv, msstats.csv, and psm.tsv from various samples.These input datsets were generated by FragPipe pipline
methods:
  1. Three key preprocess steps were applied to protein MaxLFQ intensities - filtration, normalization, and imputation, were conducted.
  2. To detect the differentially expressed protein, we took a comprehensive set of published available DEA tools, including DEqMS, DEP, proDA, ROTS, MSstat, Limma, and wilcoxon rank sum test. we also generated the T test and annova test but due to the non-normal distribution, we finnally did not use these methods
output: The outputs includes mean expressions of cases and controls, the logFC, P values and p adjusted values from various DEP pipelines
---

# loading the library
```{r setup, include=FALSE}
rm(list= ls())
library(MSnbase)

#library(diann)
# BiocManager::install("GMSimpute")
#BiocManager::install("SeqKnn")
# BiocManager::install("rrcovNA")
# BiocManager::install("NormalyzerDE")
# BiocManager::install("limma")
#BiocManager::install("DEqMS")
library(GMSimpute)
#library(SeqKnn)

library(tidyverse)
library(ggplot2)
library(ggnewscale)
library(ggrepel)
#BiocManager::install("pRoloc")

library(edgeR)
library(clusterProfiler)
library(org.Mm.eg.db)

# loading the different methods of differentially expressed analyses of proteomic datasets
library(rrcovNA)
library(NormalyzerDE)
library(limma)
library(DEqMS)
#keytypes(org.Mm.eg.db) 
# BiocManager::install("DEP")
library("DEP")
# BiocManager::install("proDA")
library(proDA)
library(MSstats)
# BiocManager::install("ROTS")
library(ROTS)

library("pRoloc")
library(enrichplot)
library(tidyverse)
library(factoextra)
#BiocManager::install("org.Hs.eg.db",force = TRUE)
#BiocManager::install("ProteoMM")
library(ProteoMM)
library(org.Hs.eg.db)

```

# set up the directory and inputs
```{r setup, include=FALSE}
## read the table

setwd("/Users/XXW004/Documents/Projects/DarylMcleod/Results/Results_03242025/Intensity_Analyses_CasesVsControls/")

OutDir <- c("/Users/XXW004/Documents/Projects/DarylMcleod/Results/Results_03242025/Intensity_Analyses_CasesVsControls/")
## set up the input dir

InputDir <- c("/Users/XXW004/Documents/Projects/DarylMcleod/Results/Results_03242025/Intensity_Analyses_CasesVsControls/")


## the input dir for clinical characteritic
ClinicInputDir<- c("/Users/XXW004/Documents/Projects/DarylMcleod/Data Analysis/")


# the input dir for Raw reads
RawInputDir <- c("/Users/XXW004/Documents/Projects/DarylMcleod/RawData/Final_Used/")
# Here we applied various statitical tools for the protein intensity results generated by Fragpipe

# the folder for the DEqMS results
# https://www.bioconductor.org/packages/release/bioc/vignettes/DEqMS/inst/doc/DEqMS-package-vignette.html
DEqMSDir <- paste0(OutDir, "DEqMS/")
# https://www.bioconductor.org/packages/devel/bioc/vignettes/DEP/inst/doc/DEP.html

# the folder for the ProteoMM #https://www.bioconductor.org/packages/release/bioc/vignettes/ProteoMM/inst/doc/ProteoMM_vignette.html
ProteoMMDir<- paste0(OutDir,"ProteoMM")
DEPDir <- paste0(OutDir, "DEP/")
# the folder for the Msstats results
# https://www.bioconductor.org/packages/release/bioc/html/MSstats.html
MSstatsDir <- paste0(OutDir, "MSstats/")
# the folder for the t-test results
TtestDir <- paste0(OutDir, "Ttest/")
# the folder for the ANNOVA results
ANNOVADir <- paste0(OutDir, "ANNOVA/")
# the folder for the Limma results
LimmaDir <- paste0(OutDir, "Limma/")
# the folder for the proDA results
proDADir<- paste0(OutDir, "proDA/")

# the folder for the ROTS results
# https://bioconductor.org/packages/devel/bioc/vignettes/ROTS/inst/doc/ROTS.pdf

ROTSDir<- paste0(OutDir,"ROTS/")
WilcoxonTestDir <- paste0(OutDir,"Wilcoxon/")

dir.create(DEqMSDir, showWarnings = FALSE)
dir.create(DEPDir, showWarnings = FALSE)
dir.create(MSstatsDir, showWarnings = FALSE)
dir.create(TtestDir, showWarnings = FALSE)
dir.create(ANNOVADir, showWarnings = FALSE)
dir.create(LimmaDir, showWarnings = FALSE)
dir.create(proDADir, showWarnings = FALSE)
dir.create(ROTSDir, showWarnings = FALSE)
dir.create(ProteoMMDir, showWarnings = FALSE)
dir.create(WilcoxonTestDir, showWarnings = FALSE)
# setting up the seed 

set.seed(1000000)


```

# we read the clinical characteristics of these samples
```{r}
# read the clinical characteristic of the sample

CaseClinicalCharacters<- read.csv(paste0(ClinicInputDir, "ClinalFactor_FromLindsey2.csv" ))

ControlClinicalCharacters<- read.csv(paste0(ClinicInputDir, "ControlClinic2.csv" ))
CaseClinicalCharacters

```

```{r}
# change the case and control id # we generated the ID 
SampleIDTransfer<- read.csv(paste0(RawInputDir,"SampleID_Transfer.csv"))

SampleIDTransfer

# For the comparison samples，we only selected the case with low eGFR vs case with normal eGFR
RequestedName<- c(paste0("Case",1:10),paste0("Case",11:20))

RequestedNameJD<- SampleIDTransfer %>% filter(ID.By.MS %in% RequestedName) %>% pull(ID.By.JD)
# 
RequestedNameJD
```

<!-- # we are using the combine_peptide.tsv to measure the minimum peptide count, Finally we did not use these part -->
<!-- ```{r} -->

<!-- df <- read.csv(paste0(RawInputDir, "combined_final_peptide_ByJF.csv"), header = TRUE, stringsAsFactors = FALSE) -->
<!-- df %>% head() -->
<!-- # Identify columns -->
<!-- protein_col <- "Protein"    -->
<!-- sample_cols <- colnames(df)[grep("Spectral.Count", colnames(df))]  # adjust pattern if needed -->
<!-- length(sample_cols) -->
<!-- df[sample_cols] -->
<!-- # Ensure samples are numeric -->
<!-- df[sample_cols] <- lapply(df[sample_cols], as.numeric) -->

<!-- # Create a presence matrix: 1 if peptide present in sample (non-zero), else 0 -->
<!-- presence_matrix <- df -->
<!-- presence_matrix[sample_cols] <- lapply(df[sample_cols], function(x) as.integer(x > 0)) -->

<!-- # Summarize: count peptides per protein per sample -->
<!-- library(dplyr) -->
<!-- peptide_counts <- presence_matrix %>% -->
<!--   group_by(across(all_of(protein_col))) %>% -->
<!--   summarise(across(all_of(sample_cols), sum), .groups = "drop") -->
<!-- peptide_counts -->

<!-- # Calculate the minimum peptide count per protein across all samples -->
<!-- peptide_counts$MinPeptides <- apply(peptide_counts[sample_cols], 1, min) -->
<!-- peptide_counts$SumPeptides <- apply(peptide_counts[sample_cols], 1, sum) -->
<!-- peptide_counts$MidPeptides <- apply(peptide_counts[sample_cols], 1, median) -->
<!-- peptide_counts %>% filter(MidPeptides >1) %>% nrow() -->
<!-- peptide_counts -->
<!-- ``` -->

<!-- # please note we used the peptide.tsv for the input to measure the minum peptide  -->
<!-- ```{r} -->
<!-- library(data.table) -->

<!-- # We read the psm.tsv from each sample and calculate the minimum PSM counts -->
<!-- # Attention, the PSM value were stored from each sample folder, we read the folder and count each protein per sample -->
<!-- psm_counts <- list() -->
<!-- PSMIndir <- c("/Users/XXW004/Library/CloudStorage/OneDrive-NationwideChildren'sHospital/Proteomics/RawProteomicDatasets/FullResults/") -->
<!-- Samplelabel<- c(paste0("Case_", 1:20), paste0("Control_",21:40)) -->
<!-- Samplelabel -->
<!-- # create a function to read the process one file -->
<!-- process_pep<- function(sample_name){ -->
<!--   #CasePsm <- fread(paste0 (PSMIndir, sample_name, "/", "psm.tsv")) -->
<!--   #sample_name<- "Case_1" -->
<!--   peptides <- read.delim(paste0 (PSMIndir, sample_name, "/", "peptide.tsv"), stringsAsFactors = FALSE) -->

<!--   # count unique peptide per protein -->
<!--   peptide_counts <- peptides %>%  -->
<!--     dplyr::select(Protein, Peptide) %>% -->
<!--   distinct() %>% -->
<!--   group_by(Protein) %>% -->
<!--   summarise(!!sample_name := n()) %>% -->
<!--   arrange(desc(sample_name)) -->
<!--   peptide_counts -->

<!--   # # Optional: remove contaminants or reverse hits -->
<!--   # CasePsm <- CasePsm[!grepl("Contaminant|Reverse", Protein),] -->
<!--   # # Step 1: Expand multi-protein assignments (split semicolon-separated IDs) -->
<!--   # psm_expanded <- separate_rows(CasePsm, Protein, sep = ";") -->
<!--   # # Step 2: Count PSMs per protein per sample -->
<!--   # name <- paste0(sample_name, "_psm") -->
<!--   # psm_expanded %>% -->
<!--   #   group_by(Protein) %>% -->
<!--   #   summarise(!!name := n(), .groups = "drop") -->
<!-- } -->

<!-- # Process all samples and combine and generate multiple lists that contain each protein and their psm value -->
<!-- psm_counts <- map(Samplelabel, process_pep) -->
<!-- psm_counts -->
<!-- # We used Reduce to merge all the lists based on the protein ID.  -->
<!-- psm_expanded <-Reduce(function(x, y) merge(x, y, by = "Protein", all = TRUE), psm_counts) -->
<!-- psm_expanded -->
<!-- # Replace NA with 0 -->
<!-- psm_expanded[is.na(psm_expanded)] <- 0 -->

<!-- colnames(psm_expanded) -->
<!-- # Calculate the minumum PSM per protein across all samples -->
<!-- psm_cols <- grep("Case|Control", colnames(psm_expanded), value = TRUE) -->
<!-- psm_cols -->
<!-- psm_expanded[,psm_cols] -->

<!-- # extract the min value for all the samples -->
<!-- psm_expanded$min_psm<-apply(psm_expanded[,psm_cols],1, function(x) min(as.numeric(x), na.rm = TRUE)) -->
<!-- psm_expanded -->
<!-- # extract the total value -->
<!-- psm_expanded$sum_psm<-apply(psm_expanded[,psm_cols],1, function(x) sum(as.numeric(x), na.rm = TRUE)) -->
<!-- # save into a psm file -->
<!-- write.csv(psm_expanded,"combine_protein_peptidenumber_updated.csv") -->

<!-- # please note that if you did not change the name, you don't need to perfom the following analyes -->
<!-- # we need to change the name into the now name -->
<!-- grep("Control_|Case_",colnames(psm_expanded),value=T) -->

<!-- #  -->
<!-- colnames(psm_expanded) -->
<!-- Old_name<- SampleIDTransfer$ID.By.JD -->
<!-- New_name<- SampleIDTransfer$ID.By.MS -->
<!-- Old_name -->
<!-- New_name -->
<!-- # create the name mapping -->
<!-- name_map<- setNames(Old_name, New_name) -->

<!-- # then apply the rename -->
<!-- psm_expanded<- psm_expanded %>% rename(!!!name_map) -->
<!-- write.csv(psm_expanded,"combine_protein_peptidenumber_rename_updated.csv") -->

<!-- # add the specific columns min values -->
<!-- psm_expanded %>% filter(min_psm >=1) %>% nrow() -->
<!-- # for selected column -->

<!-- SamplelabelSelectednlowvsnormalGFR <- c(paste0("Case", 1:10), paste0("Case",11:20)) -->
<!-- #SamplelabelSelectedlowvsnormalGFR <- c(paste0("Case", 1:20,"_psm")) -->
<!-- colnames(psm_expanded) -->
<!-- SamplelabelSelectednlowvsnormalGFR -->
<!-- psm_expanded$min_pep_lowvsnormalGFR <- apply (psm_expanded[,SamplelabelSelectednlowvsnormalGFR],1, function(x) min(x)) -->
<!-- write.csv(psm_expanded,"combine_protein_peptidenumber_rename_updated.csv") -->
<!-- psm_expanded %>% head() -->
<!-- # please attention that we need to change the Protein ID to Gene -->
<!-- # we change the ID in the DEqMS section -->
<!-- # sp|A0A024RBG1|NUD4B_HUMAN -->
<!-- ``` -->


#
# please note we are using the following PSM

Each row in combined_peptide.tsv represents a peptide (not PSM), and contains:
A unique peptide sequence
One or more associated proteins (in a column like Protein or Protein(s))
Intensity or spectral data across samples (optional)
We tried to measure the min and median peptide counts for the groups

```{r}
library(data.table)

# We read the psm.tsv from each sample and calculate the minimum PSM counts
# Attention, the PSM value were stored from each sample folder, we read the folder and count each protein per sample
psm_counts <- list()
PSMIndir <- c("/Users/XXW004/Library/CloudStorage/OneDrive-NationwideChildren'sHospital/Proteomics/RawProteomicDatasets/FullResults/")
Samplelabel<- c(paste0("Case_", 1:20), paste0("Control_",21:40))
Samplelabel
# create a function to read the process one file
process_psm<- function(sample_name){
  CasePsm <- fread(paste0 (PSMIndir, sample_name, "/", "psm.tsv"))
  # Optional: remove contaminants or reverse hits
  CasePsm <- CasePsm[!grepl("Contaminant|Reverse", Protein),]
  # Step 1: Expand multi-protein assignments (split semicolon-separated IDs)
  psm_expanded <- separate_rows(CasePsm, Protein, sep = ";")
  # Step 2: Count PSMs per protein per sample
  name <- paste0(sample_name, "_psm")
  psm_expanded %>%
    group_by(Protein) %>%
    summarise(!!name := n(), .groups = "drop")
}

# Process all samples and combine and generate multiple lists that contain each protein and their psm value
psm_counts <- map(Samplelabel, process_psm)
# We used Reduce to merge all the lists based on the protein ID.
psm_expanded <-Reduce(function(x, y) merge(x, y, by = "Protein", all = TRUE), psm_counts)

# Replace NA with 0
psm_expanded[is.na(psm_expanded)] <- 0

# Calculate the minumum PSM per protein across all samples
psm_cols <- grep("_psm", colnames(psm_expanded), value = TRUE)
psm_expanded[,psm_cols]

# extract the min value for all the samples
psm_expanded$min_psm<-apply(psm_expanded[,psm_cols],1, function(x) min(as.numeric(x), na.rm = TRUE))

# extract the median value
psm_expanded$median_psm<-apply(psm_expanded[,psm_cols],1, function(x) median(as.numeric(x), na.rm = TRUE))

# extract the total value
psm_expanded$sum_psm<-apply(psm_expanded[,psm_cols],1, function(x) sum(as.numeric(x), na.rm = TRUE))
# save into a psm file
write.csv(psm_expanded,"combine_protein_peptidenumber.csv")

# please note that if you did not change the name, you don't need to perfom the following analyes
# we need to change the name into the now name
grep("Control_|Case_",colnames(psm_expanded),value=T)

#
colnames(psm_expanded)
Old_name<- paste0(SampleIDTransfer$ID.By.JD, "_psm")
New_name<- paste0(SampleIDTransfer$ID.By.MS, "_psm")
# create the name mapping
name_map<- setNames(Old_name, New_name)
name_map
# then apply the rename
psm_expanded<- psm_expanded %>% rename(!!!name_map)
write.csv(psm_expanded,"combine_protein_peptidenumber_rename.csv")

# add the specific columns min values
psm_expanded
# for selected column

SamplelabelSelectednlowvsnormalGFR <- c(paste0("Case", 1:20,"_psm"), paste0("Case",11:20,"_psm"))
#SamplelabelSelectedlowvsnormalGFR <- c(paste0("Case", 1:20,"_psm"))
colnames(psm_expanded)
SamplelabelSelectednlowvsnormalGFR
psm_expanded$min_pep_casevsnormalGFR <- apply (psm_expanded[,SamplelabelSelectednlowvsnormalGFR],1, function(x) min(x))
psm_expanded$median_pep_casevsnormalGFR <- apply (psm_expanded[,SamplelabelSelectednlowvsnormalGFR],1, function(x) median(x))
write.csv(psm_expanded,"combine_protein_peptidenumber_rename.csv")
psm_expanded %>% head()
# please attention that we need to change the Protein ID to Gene
# we change the ID in the DEqMS section
# sp|A0A024RBG1|NUD4B_HUMAN
```

# Reading the object that proproceeded with the filteration, imputation for the MaxLFQ RDS from FragPipe
# The full preprocessing process, including filtering, normalization and immuputaion were only applied to several pipeline, including DEqMS, Limma, Wilcoxon, T test and Annova test. The other pipelines do here their own methods using the filtered results.  
```{r}
# read the original protein with spectra counts, extract the intensity values of each samples.
pro_file<-read.csv(paste0(RawInputDir,"combined_final_protein_ByJF.csv"), header = T)
head(pro_file)
length(colnames(pro_file))
colnames(pro_file)
#########################################################################################
### 1. Preprocessing the protemic dataset, including:
###             1.1 Rename the protein name:
###             1.2 Filtered the proteins that showed less than 5 samples expressed.
###             1.3 Log tranfered and normalization: here we applied the quantile normalization
#########################################################################################
#
################################################################
##  1.1 Rename the protein name: filter the proteins not detected as homo sapiens; change empty gene name to the Entry.Name and remove the _HUMAN substrings; make the protein name unique by adding _1, _2 using make.unique
### 1.1.1 we filtered the proteins that are only in the human. Note we detected some of proteins that in Ovis arises and Sus crofa, therefore, we ignore these dataset
pro_file_update<- pro_file %>% filter(Organism == "Homo sapiens") 
### 1.1.2 we change empty gene name to the Entry.Name and remove the _HUMAN substrings; make the protein name unique by adding _1, _2 using make.unique
#The dataset has unique Uniprot identifiers, however those are not immediately informative. The associated gene names are informative, however these are not always unique.
# Are there any duplicated gene names?
pro_file_update$Gene %>% duplicated() %>% any()
# Make a table of duplicated gene names
pro_file_update %>% group_by(Gene) %>% summarize(frequency = n()) %>%
  arrange(desc(frequency)) %>% filter(frequency > 1)
# Make unique names using the annotation in the "Gene.names" column as primary names and the annotation in "Protein.IDs" as name for those that do not have an gene name.

# some of genes are not well annotated, we used the entry name to fix the name
pro_file_update$Gene <- ifelse(is.na(pro_file_update$Gene) | pro_file_update$Gene == "", pro_file_update$Entry.Name, pro_file_update$Gene)
# we replace the name with _HUMAN to ''
pro_file_update$Gene<- gsub("_HUMAN","",pro_file_update$Gene)
# make the name unique
pro_file_update$Gene<- make.unique(pro_file_update$Gene)
### 1.1.3 we check the final Gene name, whether contain HUMAN, adding _1, _2 and whether they are unique
table(grepl("_HUMAN",pro_file_update$Gene))
pro_file_update$Gene %>% duplicated() %>% any()
pro_file_update %>% group_by(Gene) %>% summarize(frequency = n()) %>%
  arrange(desc(frequency)) %>% filter(frequency > 1)
# we create a column taht to add _1 _2, _3 for 
# pro_file %>%
#   group_by(Gene) %>%
#   mutate(Unique_Protein = ifelse(n() > 1, paste0(Gene, "_", row_number()), Gene))
# 
################################################################
## 1.2 Filtered the proteins that showed less than 5 samples expressed.
#  1.2.1 extract the intensity input, we used the unique intensity to make the final input
intens<-pro_file_update[, 175:length(colnames(pro_file_update))]

# put the 0 intensity value as NA
intens[intens==0]<-NA
row.names(intens) <-pro_file_update[,4]

# we remove the string .MaxLFQ.Intensity from the colnames
colnames(intens)<-gsub('.MaxLFQ.Intensity','',colnames(intens))

# we reorder the 
Desired_NameOrder <- c("Case1","Case2","Case3","Case4","Case5","Case6","Case7","Case8","Case9","Case10","Case11","Case12","Case13","Case14","Case15","Case16","Case17","Case18","Case19","Case20","Control1","Control2","Control3","Control4","Control5","Control6","Control7","Control8","Control9","Control10","Control11","Control12","Control13","Control14","Control15","Control16","Control17","Control18","Control19","Control20")
# we filtered the samples ~0.9
idx_retain<-which(apply(intens,1,function(x) sum(is.na(x)))<length(intens[1,])*0.9)
intens=intens[idx_retain,Desired_NameOrder]

# check the filter whether contain UPK2
# the requested protein
RequestedProteins<- "UPK2"
intens[RequestedProteins,]
################################################################

################################################################
## 1.3 Log tranfered and normalization
# Here we consider to use the log2 to transfer the dataset
intens_tran<-log2(as.matrix(intens))
intens_tran[RequestedProteins,]
table(is.na(intens_tran))
intens_tran[RequestedProteins,]

# as the sample have already been normalized, we did not perform the step
boxplot(intens_tran,las=2,main="PUV proteomic datasets")

# we perform the normalization by 
intens_tran_normalized = normalizeBetweenArrays(intens_tran, method="quantile")
intens_tran_normalized[RequestedProteins,]
?normalizeBetweenArrays

# We perfomed the imputation using Knn
# get the protein names:
fd <- data.frame(row.names(intens_tran_normalized))
row.names(fd)<-row.names(intens_tran_normalized)
fd
# get the sample names:
pd <- data.frame(colnames(intens_tran_normalized))
row.names(pd) = colnames(intens_tran_normalized)
pd
# imputation using the MSnSet 
intens_imp<-MSnSet(intens_tran_normalized, fd, pd)
intens_imp<- MSnbase::impute(intens_imp, "knn")

table(is.na(intens_imp))

# check the expression of these samples using the MaxLFQ density
dat.log.exp  <- intens_imp@assayData[["exprs"]]
dat.log.exp %>%  head()
# 
# 
#  # we have already log tranfer the dataset
# 
# # we then use the quanitfile normalization
 #intens_imp.vsn <- normalise(intens_imp, "vsn")
# 
# exprs(intens_imp.vsn) 
# then plot the PCA results
 intens_imp@assayData$exprs
 
# DEG_Intens before the log transfer for the raw filtered intens
library(reshape2)
IntensData<- intens %>%  rownames_to_column("ProteinID")
DEG_Intens_long<- reshape2::melt(IntensData, id.vars = "ProteinID")

DEG_Intens_long %>%  head()
# plot density

ggplot(DEG_Intens_long, aes(x = value, color = variable, fill = variable)) +
  geom_density(alpha = 0.3) +
  theme_minimal() +
  labs(title = "Density Plots for Each Sample without Log2", x = "Value", y = "Density")

# generate the density distribution after log2 and imputation
dat.log.exp  <- intens_imp@assayData[["exprs"]]
dat.log.exp
dat.log.exp.long <- reshape2::melt(dat.log.exp)


# 
dat.log.exp.long %>%  head()
ggplot(dat.log.exp.long, aes(x = value, color = Var2, fill = Var2)) +
  geom_density(alpha = 0.3) +
  theme_minimal() +
  labs(title = "Density Plots for Each Sample with Log2 and imputation", x = "Value", y = "Density")
 
##### generate a PCA analysese
dev.off()
colorDiff<-c(rep("red",20), rep("blue",20))

mds <- plotMDS(dat.log.exp, col=colorDiff)
toplot <- data.frame(Dim1 = mds$x, Dim2 = mds$y)
library(ggplot2)
#ggplot(toplot, aes(Dim1, Dim2))+ geom_point() +
# geom_text_repel(label=label_name,segment.size=0.01,hjust=0.2, size=3,  col=c(rep("gray",5),rep("black",4),rep("blue",4), rep("red",4), rep("yellow",4),rep("violet",4),rep("brown",4),rep("blue",4),))+
#  theme_classic()
#geom_text( label = label_name, nudge_x = 0.1, nudge_y = 0.1,  check_overlap = T)+
#theme_bw() + 
#  theme(panel.border = element_blank(), panel.grid.major = element_blank(),panel.grid.minor = element_blank(), axis.line = element_line(colour = "black"))
#Gcol<-c(rep("gray",5),rep("black",4),rep("blue",4), rep("red",4),rep("brown",4),rep("yellow",4),rep("navy blue",3), rep("purple",4),rep("LimeGreen",4),rep("pink",4),rep("orange",4), rep("magenta",4))
pdf("PCAanalyses.pdf", width = 6, height = 6)
ggplot(toplot, aes(Dim1, Dim2))+ geom_point(col=colorDiff) +
  geom_text_repel(label=colnames(dat.log.exp),segment.size=0.01,hjust=0.2, size=3, col=colorDiff,max.overlaps=20)+
  theme_classic()+
  #geom_text( label = label_name, nudge_x = 0.1, nudge_y = 0.1,  check_overlap = T)+
  theme_bw() + 
  theme(panel.border = element_blank(), panel.grid.major = element_blank(),panel.grid.minor = element_blank(), axis.line = element_line(colour = "black"))

dev.off()

dat.log.exp = na.omit(dat.log.exp)
# we used the median centered
boxplot(dat.log.exp, main ="PUV proteomic")

# We generate the final expressed dataset, this dataset have already performed the filtering, pimputation. 
str(dat.log.exp)
```

# Differentially expressed proteins detection using various pipelines, including DEqMS, Limma, Wilxcon, DEP, proDA, ROTS.

# run with the DEqMS and Limma
```{r}
# 2. the method of DEqMS
# Here we used quant data columns for DEqMS
# the 
setwd(dir = DEqMSDir)
# 2.1.1 preprocess the data
# due to the data is not median centered, we normalized the based on median normalization
#Here we used the filtered datasets
dat.log.exp %>% head()

boxplot(dat.log.exp, main ="PUV proteomic")
rownames(dat.log.exp)
# 2.1.2 Make design table
cond = as.factor(c(rep("Case", 20), rep("Controls",20)))

# the function mdoel is use the generate the design matrix
design = model.matrix(~0+ cond)
colnames(design) = gsub("cond","",colnames(design))
colnames(design)

# 2.1.3 make contrasts
x<- c("Case-Controls")
contrast =  makeContrasts(contrasts=x,levels=design)
contrast
fit1 <- lmFit(dat.log.exp, design)
fit2 <- contrasts.fit(fit1,contrasts = contrast)
fit3 <- eBayes(fit2)
print(fit3)
topTable(fit3)

# the limma results
FinalLimmaResults<- topTable(fit3, coef=1, adjust="fdr", number=Inf)

write.csv(FinalLimmaResults, paste(LimmaDir,"PUV_Limma_results_PUVvsControl.csv"))

# The above shows Limma part, now we use the function spectraCounteBayes in DEqMS to correct bias of variance estimate based on minimum number of psms per protein used for quantification.We use the minimum number of PSMs used for quantification within and across experiments to model the relation between variance and PSM count.(See original paper)
# 2.1.4 Integrate peptide count information for DEqMS analysis

# assign a extra variable `count` to fit3 object, telling how many PSMs are quantifed for each protein
# Here we used the peptide counts, note that it is not the spetrum counts
library(matrixStats)

# check the protein number in each sample
# The following add the gene name based on the protein ID for the PSM counts (psm_expanded)
ProteinGeneTransfer<- pro_file_update[,c(1,4)]
psm_expanded <- psm_expanded %>% left_join(ProteinGeneTransfer, by = "Protein")

write.csv(psm_expanded, file = "combine_protein_peptidenumber_rename_updated_gene.csv")
# please note that we should only keep the proteins that used for the limma analyses.
count_table= psm_expanded %>% filter(Gene %in% rownames(fit3)) 
rownames(count_table) = count_table$Gene
length(count_table$min_psm) ## to be consistent.

# set up the protein counts.
fit3$count = count_table[rownames(fit3$coefficients),"median_psm"]

# Remove missing spectral counts

dim(fit3$coefficients) 
# try adding a pseudo count 1 to all values before using spectraCounteBayes function.if the minimum PSM count is 0, you can add a pseudo count 1 to it.
fit3$count=  fit3$count+1
# Remove missing spectral counts

dim(fit3$coefficients) 

fit4 = spectraCounteBayes(fit3)
#VarianceBoxplot(fit4)
# extract the results as data frame and svae it
# the DEqMS results
DEqMS.results = outputResult(fit4,coef_col = 1)
DEqMS.results %>% head()

DEqMS.results %>% dplyr::filter(sca.P.Value <=0.05) %>% nrow()
# the above method is not a good way to detect the DEPs using the peptide

DEqMS.results$log.sca.pval = -log10(DEqMS.results$sca.P.Value)
ggplot(DEqMS.results, aes(x = logFC, y =log.sca.pval )) + 
    geom_point(size=0.5 )+
    theme_bw(base_size = 16) + # change theme
    xlab(expression("log2(miR372/ctrl)")) + # x-axis label
    ylab(expression(" -log10(P-value)")) + # y-axis label
    geom_vline(xintercept = c(-1,1), colour = "red") + # Add fold change cutoffs
    geom_hline(yintercept = 3, colour = "red") + # Add significance cutoffs
    geom_vline(xintercept = 0, colour = "black") + # Add 0 lines
    scale_colour_gradient(low = "black", high = "black", guide = FALSE)+
    geom_text_repel(data=subset(DEqMS.results, abs(logFC)>1&log.sca.pval > 3),
                    aes( logFC, log.sca.pval ,label=gene)) # add gene label

fit4$p.value = fit4$sca.p
# volcanoplot highlight top 20 proteins ranked by p-value here
volcanoplot(fit4,coef=1, style = "p-value", highlight = 20,
            names=rownames(fit4$coefficients))
#VarianceBoxplot(fit4,n=30,main="TMT10plex dataset PXD004163",xlab="PSM count")
VarianceScatterplot(fit4,main="TMT10plex dataset PXD004163")
# comparing DEqMS to other methods
VarianceScatterplot(fit3, xlab="log2(PSM count)")
limma.prior = fit3$s2.prior
abline(h = log(limma.prior),col="green",lwd=3 )
legend("topright",legend=c("DEqMS prior variance","Limma prior variance"),
        col=c("red","green"),lwd=3)

op <- par(mfrow=c(1,2), mar=c(4,4,4,1), oma=c(0.5,0.5,0.5,0))
Residualplot(fit3,  xlab="log2(PSM count)",main="DEqMS")
x = fit3$count
y = log(limma.prior) - log(fit3$sigma^2)
plot(log2(x),y,ylim=c(-6,2),ylab="Variance(estimated-observed)", pch=20, cex=0.5,
     xlab = "log2(PSMcount)",main="Limma")
# Posterior variance comparison between DEqMS and Limma
library(LSD)
op <- par(mfrow=c(1,2), mar=c(4,4,4,1), oma=c(0.5,0.5,0.5,0))
x = fit3$count
y = fit3$s2.post
heatscatter(log2(x),log(y),pch=20, xlab = "log2(PSMcount)", 
     ylab="log(Variance)",
     main="Posterior Variance in Limma")

y = fit3$sca.postvar
heatscatter(log2(x),log(y),pch=20, xlab = "log2(PSMcount)",
     ylab="log(Variance)", 
     main="Posterior Variance in DEqMS")

write.csv(DEqMS.results, "PUV_DEqMStest_results_PUVvsControl.csv")

```

# T test
```{r}
setwd(TtestDir)
# in order to use T test, we test for nromality
shapiro.test(dat.log.exp[,3])
# apply the shapiro test to all sample
# clearly the T test is 
shapiro_results <- apply(dat.log.exp, 2, function(x) shapiro.test(x)$p.value)
shapiro_results
# As the data is not normally distributed, we will not consider parameteric test that assume normality (e.g., t-test, ANOVA) may not be appropriate. Our datasets showed clearly biomodal, we therefore applied Wilcoxon Rank sum test instead 


dat.log.exp %>% head()
pval.372 = apply(dat.log.exp, 1, function(x) 
t.test(as.numeric(x[c(1:20)]), as.numeric(x[c(21:40)]))$p.value)

logFC.372 = rowMeans(dat.log.exp[,c(1:20)])-rowMeans(dat.log.exp[,c(21:40)])


ttest.results = data.frame(gene=rownames(dat.log.exp),
                    logFC=logFC.372,P.Value = pval.372, 
                    adj.pval = p.adjust(pval.372,method = "fdr")) 
ttest.results
#ttest.results$PSMcount = psm.count.table[ttest.results$gene,"count"]
ttest.results = ttest.results[with(ttest.results, order(P.Value)), ]
head(ttest.results)
# save the results 
write.csv(ttest.results, "PUV_Ttest_results_PUVvsControl.csv")

```

# Wilcoxon Rank Test for them
```{r}
setwd(WilcoxonTestDir)
# in order to use T test, we test for nromality
shapiro.test(dat.log.exp[,3])
# apply the shapiro test to all sample
# clearly the T test is 
shapiro_results <- apply(dat.log.exp, 2, function(x) shapiro.test(x)$p.value)

# As the data is not normally distributed, we will not consider parameteric test that assume normality (e.g., t-test, ANOVA) may not be appropriate. Our datasets showed clearly biomodal, we therefore applied Wilcoxon Rank sum test instead 


dat.log.exp %>% head()
pval.wilcox = apply(dat.log.exp, 1, function(x) 
wilcox.test(as.numeric(x[c(1:20)]), as.numeric(x[c(21:40)]))$p.value)

logFC.372 = rowMeans(dat.log.exp[,c(1:20)])-rowMeans(dat.log.exp[,c(21:40)])


wilcox.results = data.frame(gene=rownames(dat.log.exp),
                    logFC=logFC.372,P.Value = pval.wilcox, 
                    adj.pval = p.adjust(pval.wilcox,method = "fdr")) 
wilcox.results
#ttest.results$PSMcount = psm.count.table[ttest.results$gene,"count"]
wilcox.results = wilcox.results[with(wilcox.results, order(P.Value)), ]

# we inner join two lists wilcox.results and dat.log.exp
Finalwilcox.results<- wilcox.results %>%  rownames_to_column("ID") %>% inner_join(dat.log.exp %>% as.data.frame() %>%  rownames_to_column("ID"), by ="ID") 

head(Finalwilcox.results)
# save the results 
write.csv(Finalwilcox.results, "PUV_Wilcoxtest_results_PUVvsControl.csv")
```

# Annova analysis
Anova analysis is equivalent to linear model analysis. The difference to Limma analysis is that estimated variance is not moderated using empirical bayesian approach as it is done in Limma.
```{r}
# set up the path:
setwd(ANNOVADir)
ord.t = fit1$coefficients[, 1]/fit1$sigma/fit1$stdev.unscaled[, 1]
ord.p = 2*pt(abs(ord.t), fit1$df.residual, lower.tail = FALSE)
ord.q = p.adjust(ord.p,method = "BH")
anova.results = data.frame(gene=names(fit1$sigma),
                            logFC=fit1$coefficients[,1],
                            t=ord.t, 
                            P.Value=ord.p, 
                            adj.P.Val = ord.q)
anova.results
#psm.count.table
#anova.results$PSMcount = psm.count.table[anova.results$gene,"count"]
anova.results = anova.results[with(anova.results,order(P.Value)),]
head(anova.results)

write.csv(anova.results, file = "PUV_Annovar_results_PUVvsControl.csv")
```

<!-- # Limma -->
<!-- Extract limma results using topTable function, coef = 1 allows you to extract the specific contrast (miR372-ctrl), option n= Inf output all rows. -->
<!-- ```{r} -->

<!-- limma.results = topTable(fit2,coef = 1,n= Inf) -->
<!-- limma.results$gene = rownames(limma.results) -->
<!-- #Add PSM count values in the data frame -->
<!-- limma.results$PSMcount = psm.count.table[limma.results$gene,"count"] -->

<!-- head(limma.results) -->
<!-- ``` -->

# Visualize the distribution of p-values by different analysis
plotting all proteins ranked by p-values.

```{r}
#DEqMS.results
plot(sort(-log10(FinalLimmaResults$P.Value),decreasing = TRUE), 
    type="l",lty=2,lwd=2, ylab="-log10(p-value)",ylim = c(0,40),
    xlab="Proteins ranked by p-values",
    col="purple")
lines(sort(-log10(DEqMS.results$sca.P.Value),decreasing = TRUE), 
        lty=1,lwd=2,col="red")
lines(sort(-log10(anova.results$P.Value),decreasing = TRUE), 
        lty=2,lwd=2,col="blue")
lines(sort(-log10(ttest.results$P.Value),decreasing = TRUE), 
        lty=2,lwd=2,col="orange")
legend("topright",legend = c("Limma","DEqMS","Anova","t.test"),
        col = c("purple","red","blue","orange"),lty=c(2,1,2,2),lwd=2)
dev.off()

# top 500 proteins
plot(sort(-log10(FinalLimmaResults$P.Value),decreasing = TRUE)[1:500], 
    type="l",lty=2,lwd=2, ylab="-log10(p-value)", ylim = c(2,10),
    xlab="Proteins ranked by p-values",
    col="purple")
lines(sort(-log10(DEqMS.results$sca.P.Value),decreasing = TRUE)[1:500], 
        lty=1,lwd=2,col="red")
lines(sort(-log10(anova.results$P.Value),decreasing = TRUE)[1:500], 
        lty=2,lwd=2,col="blue")
lines(sort(-log10(ttest.results$P.Value),decreasing = TRUE)[1:500], 
        lty=2,lwd=2,col="orange")
legend("topright",legend = c("Limma","DEqMS","Anova","t.test"),
        col = c("purple","red","blue","orange"),lty=c(2,1,2,2),lwd=2)

```

# DEP Analyses 
https://www.bioconductor.org/packages/devel/bioc/vignettes/DEP/inst/doc/DEP.html
# Attention: DEP will use the raw MaxLFQ intensity results
```{r}
# Data prepration for the DEP, we ignored the remove duplicated proteins and 
setwd(dir = DEPDir)

# Here we also use the orginal MAX LFQ as input
intens %>% head()
#as.data.frame(dat.log) %>% rownames_to_column("ID") %>% head()
DEG_Intens<- as.data.frame(intens) %>% rownames_to_column("ID") 
# get LFQ column number
LFQ_columns_number <- c(2:41)
# check the distribution
DEG_Intens$Case1

# create a data frame for experimental design
experimental_design <- data.frame(
  label = colnames(DEG_Intens)[2:41],  # Sample names
  condition = as.factor(c(rep("Case", 20), rep("Controls",20))),  # Define conditions
  replicate = c(seq(1:20), seq(1:20)) # Biological replicates
)

DEG_Intens$name<- DEG_Intens[,1]

?make_se

data.se <- make_se(DEG_Intens, LFQ_columns_number, experimental_design)
plot_frequency(data.se)
dim(data.se)
colnames(data.se)
# Filter for proteins that are identified in 4 replicates of at least one condition
data_filt <- filter_missval(data.se, thr = 4) # we did not use this filtering as we already filering the datasets
?filter_missval

#plot_numbers(data_filt)

#plot_coverage(data_filt)
# Plot intensity distributions and cumulative fraction of proteins with and without missing values
#plot_detect(data_filt)
# Less stringent filtering:
# Filter for proteins that are identified in 2 out of 3 replicates of at least one condition
# data_filt2 <- filter_missval(data.se, thr = 1)

# Normalization, we did not filter the proteins
data_norm <- normalize_vsn(data.se)
# Visualize normalization by boxplots for all samples before and after normalization
plot_normalization(data_filt, data_norm)

# Impute data for missing values 
plot_missval(data_filt)
# All possible imputation methods are printed in an error, if an invalid function name is given.

# Impute missing data using the k-nearest neighbour approach (for MAR)
data_imp_knn <- DEP::impute(data_norm, fun = "knn", rowmax = 0.9)

plot_imputation(data_norm, data_imp_knn)

# Differential enrichment analysis  based on linear models and empherical Bayes statistics

# Test every sample versus control
data_diff <- DEP::test_diff(data_imp_knn, type = "all")
data_diff %>% head()
dep <- add_rejections(data_diff, alpha = 0.05, lfc = 0)
dep
# Get the results
res <- get_results(dep)

res %>% head()
# Visualization of the results
plot_pca(dep, x = 1, y = 2, n = 500, point_size = 4)
# Plot the Pearson correlation matrix
plot_cor(dep, significant = TRUE, lower = 0, upper = 1, pal = "Reds")

plot_heatmap(dep, type = "centered", kmeans = TRUE, 
             k = 6, col_limit = 4, show_row_names = FALSE
             )

plot_heatmap(dep, type = "contrast", kmeans = TRUE, 
             k = 6, col_limit = 10, show_row_names = FALSE)
plot_volcano(dep, contrast = "Case_vs_Controls", label_size = 2, add_names = TRUE)

# Barplots of a protein of interest
#plot_single(dep, proteins = c("USP15", "IKBKG"))

# generate the results table
# Generate a results table
data_results <- get_results(dep)

# order by the p value
data_results = data_results[with(data_results,order(Case_vs_Controls_p.val)),]
data_results %>% head()
write.csv(data_results,"PUV_DEP_results_PUVvsControl.csv")

```
      
# MSstats: 
https://www.bioconductor.org/packages/release/bioc/vignettes/MSstats/inst/doc/MSstatsWorkflow.html

We used the MSstats using MSstats.csv generated by Fragpipe.
https://fragpipe.nesvilab.org/docs/tutorial_msstats.html
```{r}
# set up the path:
setwd(MSstatsDir)

# please attention the replicate id is not consistent with the sample id before
raw <-
  read_csv(paste0(RawInputDir, "MSstats.csv"), na = c("", "NA", "0"))
# we read the Msstats.csv
raw$ProteinName <- factor(raw$ProteinName)
raw$PeptideSequence <- factor(raw$PeptideSequence)

# Processing the adata using MSstats

processedData <- dataProcess(raw, logTrans = 10)

? dataProcess
head(processedData$FeatureLevelData)
head(processedData$ProteinLevelData)

# Data process Plots
dataProcessPlots(
  data = processedData,
  type = "ProfilePlot",
  address = FALSE,
  which.Protein = "sp|P12109|CO6A1_HUMAN"
)
# Quality control plot
dataProcessPlots(
  data = processedData,
  type = "QCPlot",
  address = FALSE,
  which.Protein = "sp|P12109|CO6A1_HUMAN"
)
# Quantification plot for conditions
dataProcessPlots(
  data = processedData,
  type = "ConditionPlot",
  address = FALSE,
  which.Protein = "sp|P12109|CO6A1_HUMAN"
)

# Modeling
# In this step we test for differential changes in protein abundance across conditions using a linear mixed-effects model. The model will be automatically adjusted based on your experimental design.
#
# A contrast matrix must be provided to the model. Alternatively, all pairwise comparisons can be made by passing pairwise to the function. For more information on creating contrast matrices, please see the citation linked at the beginning of this document.
model = groupComparison("pairwise", processedData)
head(model$ModelQC)
head(model$ComparisonResult)

model$ComparisonResult %>% head()

# rank the results by the Pvalue

MSstasResults <- model$ComparisonResult %>% arrange(pvalue)

# we need to change the protein name into symbol
pro_file_update[, c(1, 4)] %>% head()

MSstasResultsF <-
  MSstasResults %>% inner_join(pro_file_update[, c(1, 4)], by = "Protein")
nrow(MSstasResultsF)
MSstasResultsF %>% head()
# save into the file

write.csv(MSstasResultsF, "PUV_MSstats_results_PUVvsControl.csv")

```


# proDA
Here we will use proDA to differentially abundant proteins in label-free mass spectrometry data. The main challenge of this data are the many missing values. The missing values don’t occur randomly but especially at low intensities. This means that they cannot just be ignored. Existing methods have mostly focused on replacing the missing values with some reasonable number (“imputation”) and then run classical methods. But imputation is problematic because it obscures the amount of available information. Which in turn can lead to over-confident predictions.
https://www.bioconductor.org/packages/release/bioc/vignettes/proDA/inst/doc/Introduction.html
```{r}
library(proDA)

setwd(proDADir)

# We used the preliminary datasets that has been already well structured
intens_tran ### There are lots of NA values, and have already log2 transfered

# Quality control 
barplot(colSums(is.na(intens_tran)),
        ylab = "# missing values",
        xlab = "Sample 1 to 40")
boxplot(intens_tran,
        ylab = "Intensity Distribution",
        xlab = "Sample 1 to 36")
# Note that, the intensity distribution is shifted upwards for samples which also have a large number of missing values (for example the last one). This agrees with our idea that small values are more likely to be missing. On the other hand, this also demonstrates why normalization methods such as quantile normalization, which distort the data until all the distributions are equal, are problematic. I will apply the more “conservative” median normalization, which ignores the missing values and transforms the values so that the median difference between the sample and average across all other samples is zero.
normalized_abundance_matrix <- median_normalization(intens_tran)
# The base R dist() function can not handle input data that contains missing values, so we might be tempted to just replace the missing values with some realistic numbers and calculate the distance on the completed dataset. But choosing a good replacement value is challenging and can also be misleading because the samples with many missing values would be considered too close.

# Instead proDA provides the dist_approx() function that takes either a fitted model (ie. the output from proDA()) or a simple matrix (for which it internally calls proDA()) and estimates the expected distance without imputing the missing values. In addition, it reports the associated uncertainty with every estimate. The estimates for samples with many missing values will be uncertain, allowing the data analyst to discount them.
da <- dist_approx(normalized_abundance_matrix) # dist_approx() returns two elements the mean of the estimate and the associated sd. In the next step I will plot the heatmap for three different conditions, adding the 95% confidence interval as text to each cell.
# check the row name

# Fit the probablitistic dropout model
# In the next step, we will fit the actual linear probabilistic dropout model to the normalized data. But before we start, I will create a data.frame that contains some additional information on each sample, in particular to which condition that sample belongs
colnames(normalized_abundance_matrix)
sample_info_df <- data.frame(
  name = colnames(normalized_abundance_matrix),  # Sample names
  condition = as.factor(c(rep("Case", 20), rep("Controls",20))),  # Define conditions
  replicate = c(seq(1:20), seq(1:20)) # Biological replicates
)

# Now we can call the proDA() function to actually fit the model. We specify the design using the formula notation, referencing the condition column in the sample_info_df data.frame that we have just created. In addition, I specify that I want to use the S2R condition as the reference because I know that it was the negative control and this way automatically all coefficients measure how much each condition differs from the negative control.

fit <- proDA(normalized_abundance_matrix, design = ~ condition, 
             col_data = sample_info_df, reference_level = "Controls")

fit
# Equivalent to feature_parameters(fit)
fit$feature_parameters %>% head()

# Identify differential abundance
test_res <- test_diff(fit, "conditionCase")
proDAResults<- test_res %>% arrange(pval)


# save into the file

write.csv(proDAResults, "PUV_proDA_results_PUVvsControl.csv")

```

# ROTS 
```{r}
library(ROTS)

setwd(ROTSDir)

data("upsSpikeIn")

# we used the normalized datasets
dat.log.exp # with the imputation and normalization

groups =c(rep(0,20), rep(1,20))
results = ROTS(data = dat.log.exp, groups = groups , B = 10000 , K = 500 , seed = 1234)

names(results)
summary(results, fdr = 0.05)
plot(results, fdr = 0.05, type = "volcano")
plot(results, fdr = 0.05, type = "heatmap")


# save results into 
names(results)

# add the output into a data.frame
rownames(results$data)
ROTS.results = data.frame(gene=rownames(results$data),
                    logFC=results$logfc,P.Value = results$pvalue, 
                    adj.pval = results$FDR) 
ROTS.results<-ROTS.results %>% arrange(P.Value) 
saveRDS(results, "PUV_ROTS_results_PUVvsControl.rds")

write.csv(ROTS.results, "PUV_ROTS_results_PUVvsControl.csv")
```

